# Local Evaluation Pipeline Setup
To debug and test your adaptor solution in our evaluation pipeline locally, follow these steps to configure your environment correctly:

## 1. Build the Evaluation Container
First, clone this repo and build the evaluation Docker container locally by running the provided build script:
[do\_build.sh](https://github.com/DIAGNijmegen/unicorn_eval/blob/add-local-debugging/tests/do_build.sh)

## 2. Prepare the Input Folder
Your input data, i.e., the output produced by your algorithm Docker container, should follow this folder structure:

```
input/
├── pk-value/
│   └── output/
│       └── patch-or-image-neural-representation.json
├── pk-value/
│   └── output/
│       └── patch-or-image-neural-representation.json
├── predictions.json
├── adaptor-task-file.json       // e.g. adaptor-pathology-classification.json
```
* **`pk-value`**: Each folder corresponds to a unique case identifier (primary key).
* **`patch-neural-representation.json`**/ **`image-neural-representation.json`**: Contains the neural representations produced by your algorithm for the case.
* **`predictions.json`**: Automatically generated by the platform, maps each platform-generated primary key (`pk`) to the corresponding local `case ID`.
* **`tests/task-1/input/adaptor-pathology-classification.json`**: Specifies which adaptor method to use, e.g., `"1-nn"`, `"linear-probing"`, `"mlp"`, etc.

**Reference example files**

You can find working examples of these files in the `tests` folder of this repository:
* [`tests/task-1/input/predictions.json`](task-1/input/predictions.json)
* [`tests/task-1/input/adaptor-pathology-classification.json`](task-1/input/adaptor-pathology-classification.json)
* [`tests/task-1/input/0403dcc49b1420545299f692f7d8e270/output/image-neural-representation.json`](task-1/input/0403dcc49b1420545299f692f7d8e270/output/image-neural-representation.json)

You can also download `patch-neural-representation.json` or `image-neural-representation.json` from Grand Challenge after executing your algorithm on a leaderboard. Downloading these files will help you understand its structure. Or, alternative, create them yourself by running your algorithm locally.

The simplified `predictions.json` file follows this structure, for local debugging, you'll need to modify this file so that it includes all `case IDs` from `mapping.csv`, with each `pk` corresponding to a local folder name.
```
[
  {
    "pk": <pk-value>,       // pk-value is same as case_id
    "inputs": [
      {
        "image": {
          "name": <case_id>       // matches case IDs in mapping.csv
        },
        "interface": {
          "slug": <input_slug>       // retrieve the input slug for each task from INPUT_SLUGS_DICT (defined in evaluate.py)
        }
      }
    ],
    "outputs": [
      {
        "interface": {
          "slug": <neural-representation-type>,       // either "image-neural-representation" or "patch-neural-representation"
          "relative_path": <json-file-name>           // either "image-neural-representation.json" or "patch-neural-representation.json"
        }
      }
    ]
  }
]
```

The `adaptor-task-file.json` file is a task-specific file that indicates which adaptor method to use. Its filename (e.g., adaptor-pathology-classification.json) can be retrieved from ADAPTOR_SLUGS_DICT (defined in [`evaluate.py`](../src/unicorn_eval/evaluate.py)). The content of this file is the adaptor name, which can be selected when submitting to a task on Grand Challenge—such as "1-nn", "linear-probing", "mlp", ... .

## 3. Ground Truth Setup
The evaluation also requires ground truth data, structured in the following way:

``` 
/opt/ml/input/data/ground_truth/
└── <task_name>       // e.g. Task01_classifying_he_prostate_biopsies_into_isup_scores
    └── pk-value
        └── label       // e.g. isup-grade.json 
        └── (optional additional input)       // This input is needed for Task 3 (event.json, can be found on Zenodo) and Task 7 (will be added later)
├── mapping.csv
```
* **`mapping.csv`** maps case IDs to relevant task metadata, such as task_name and domain, and specifies whether the case is used for training (“shot”) or testing (“case”).
* **`label`** is the ground truth as is shown on Zenodo.

**Important:** The `name` field in `predictions.json` must exactly match the case ID used in `mapping.csv`. If they don't align, the evaluation container will be unable to associate predictions with the correct ground truth data.

To generate your own `mapping.csv`, we provided [this script](generate_mapping.py). An example of the `mapping.csv` structure can be found below:
```
case_id,task_name,task_type,domain,modality,split
<case_id>,<task_name>,classification/detection/segmentation,pathology/CT/MR,vision,shot/case
```

**Reference example files**
* [`tests/task-1/ground-truth/mapping.csv`](task-1/ground-truth/mapping.csv)
* [`tests/task-1/ground-truth/Task01_classifying_he_prostate_biopsies_into_isup_scores/0403dcc49b1420545299f692f7d8e270/isup-grade.json`](task-1/ground-truth/Task01_classifying_he_prostate_biopsies_into_isup_scores/0403dcc49b1420545299f692f7d8e270/isup-grade.json)


## 3. Running the Evaluation Locally
- Obtain the neural representation `JSON` files by either:
  - Using the [task 1 examples from this repo](task-1/input)  **or**
  - Downloading them from Grand Challenge when navigating to `Submit` -> `Submissions` -> your algorithm run -> `Result` or navigate to your algorithm page -> `Results` after running your algorithm on the leaderboard, **or**
  - Running your algorithm locally on public data from Zenodo ([baseline setup example](https://github.com/DIAGNijmegen/unicorn_baseline/blob/main/setup-docker.md)).
- Ensure that:
  - The input folder structure matches the specification above.
  - Case IDs align exactly across `predictions.json`, `mapping.csv`, and folder names.
- Use the evaluation container or the provided ``do_test_run.sh`` script to run the evaluation locally and verify that everything works correctly. Usage:
```
./tests/do_test_run.sh <INPUT_FOLDER> <GROUND_TRUTH_FOLDER> <OUTPUT_FOLDER> [DOCKER_IMAGE_TAG]
```

    - INPUT_FOLDER: Path to the folder containing predictions (should follow structure above).
    - GROUND_TRUTH_FOLDER: Folder containing mapping.csv and ground truth JSON files (should follow structure above).
    - OUTPUT_FOLDER: Where evaluation results are saved.
    - DOCKER_IMAGE_TAG: (optional) Defaults to unicorn_eval.

## Example Data and Tips
- If you're using your own algorithm or working on a task other than Task 1, the [public few-shot example data on Zenodo](https://doi.org/10.5281/zenodo.14832502) is a great starting point. These datasets provide sample inputs and labels for multiple tasks, allowing you to set up and test your local environment effectively.
- We recommend creating a custom `mapping.csv` using the provided template, and defining your own data split. For instance, you might allocate 30% of the few-shot examples as "shot" and the remaining 70% as "case" to simulate different experimental conditions.