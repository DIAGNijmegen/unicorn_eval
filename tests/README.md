# Local Evaluation Pipeline Setup
To debug and test your adaptor solution in our evaluation pipeline locally, follow these steps to configure your environment correctly:

## 1. Build the Evaluation Container
First, clone this repo and build the evaluation Docker container locally by running the provided build script:
[do\_build.sh](https://github.com/DIAGNijmegen/unicorn_eval/blob/add-local-debugging/tests/do_build.sh)

## 2. Prepare the input
Your input data, i.e., the output produced by your algorithm Docker container, should follow this folder structure:

```
input/
├── <pk-value>/
│   └── output/
│       └── patch-or-image-neural-representation.json
├── predictions.json
├── adaptor-task-file.json       // e.g. adaptor-pathology-classification.json
```
* **`pk-value`**: Each folder corresponds to a unique case identifier (primary key).
* **`patch-neural-representation.json`**/ **`image-neural-representation.json`**: Contains the neural representations produced by your algorithm for the case.
* **`predictions.json`**: Automatically generated by the platform, maps each platform-generated primary key (`pk`) to the corresponding local `case ID`.
* **`adaptor-task-file.json`**: Specifies which adaptor method to use, e.g., `"1-nn"`, `"linear-probing"`, `"mlp"`, etc.

**Reference example files**

You can find working examples of these files in the `tests` folder of this repository:
* [`tests/vision/input/predictions.json`](vision/input/predictions.json)
* [`tests/vision/input/adaptor-pathology-classification.json`](vision/input/adaptor-pathology-classification.json)
* [`tests/vision/input/0403dcc49b1420545299f692f7d8e270/output/image-neural-representation.json`](vision/input/0403dcc49b1420545299f692f7d8e270/output/image-neural-representation.json)

You can also download `patch-neural-representation.json` or `image-neural-representation.json` from Grand Challenge after executing your algorithm on a leaderboard. Downloading these files will help you understand their structure. Or, alternatively, create them yourself by running your algorithm locally.

The simplified `predictions.json` is structured as follows:
```
[
  {
    "pk": <pk-value>,       // pk-value is same as case_id
    "inputs": [
      {
        "image": {
          "name": <case_id>       // matches case IDs in mapping.csv
        },
        "interface": {
          "slug": <input_slug>       // retrieve the input slug for each task from INPUT_SLUGS_DICT (defined in evaluate.py)
        }
      }
    ],
    "outputs": [
      {
        "interface": {
          "slug": <neural-representation-type>,       // either "image-neural-representation" or "patch-neural-representation"
          "relative_path": <json-file-name>           // either "image-neural-representation.json" or "patch-neural-representation.json"
        }
      }
    ]
  }
]
```

For local debugging, edit this file so that it includes all case ids from the `mapping.csv`.

The `adaptor-task-file.json` file is a task-specific file that indicates which adaptor method to use. Its filename (e.g., adaptor-pathology-classification.json) can be retrieved from ADAPTOR_SLUGS_DICT (defined in [`evaluate.py`](../src/unicorn_eval/evaluate.py)). The content of this file is the adaptor name, which can be selected when submitting to a task on Grand Challenge (e.g. "linear-probing", "mlp", ...).

## 3. Prepare the groundtruth
The evaluation also requires ground truth data, structured as follows:

``` 
/opt/ml/input/data/ground_truth/
└── <task_name>       // e.g. Task01_classifying_he_prostate_biopsies_into_isup_scores
    └── <pk-value>
        └── label       // e.g. isup-grade.json 
        └── (optional additional input)       // This input is needed for Task 3 (event.json, can be found on Zenodo) and Task 7 (will be added later)
├── mapping.csv
```
* **`mapping.csv`** maps case IDs to relevant task metadata, such as task_name and domain, and specifies whether the case is used for training (“shot”) or testing (“case”).
* **`label`** is the ground truth as is shown on Zenodo.

**Important:** The `name` field in `predictions.json` must exactly match the case id used in `mapping.csv`. If they don't align, the evaluation container will be unable to associate predictions with the correct ground truth data.

This repository comes with a `mapping.csv` and grountruth files based on the public few-shots released on Zenodo, ready to be used for local debugging:

* [`tests/vision/ground_truth/mapping.csv`](vision/ground_truth/mapping.csv)
* [`tests/vision/ground_truth/Task01_classifying_he_prostate_biopsies_into_isup_scores/0403dcc49b1420545299f692f7d8e270/isup-grade.json`](vision/ground_truth/Task01_classifying_he_prostate_biopsies_into_isup_scores/0403dcc49b1420545299f692f7d8e270/isup-grade.json)

If you need to generate your own `mapping.csv`, we provide the following [script](generate_mapping.py). 

## 4. Running the Evaluation Locally
- Obtain the neural representation `JSON` files by either:
  - Using the [examples from this repo](vision/input)  (will only work if you do not use a custom adaptor) **or**
  - Downloading them from Grand Challenge when navigating to `Submit` -> `Submissions` -> your algorithm run -> `Result` or navigate to your algorithm page -> `Results` after running your algorithm on the leaderboard, **or**
  - Running your algorithm locally on public data from Zenodo ([baseline setup example](https://github.com/DIAGNijmegen/unicorn_baseline/blob/main/setup-docker.md)).
- Ensure that:
  - The input folder structure matches the specification above.
  - Case ids align exactly across `predictions.json`, `mapping.csv`, and folder names.
- Use the evaluation container or the provided ``do_test_run.sh`` script to run the evaluation locally and verify that everything works correctly. Usage:
```
./tests/do_test_run.sh <INPUT_FOLDER> <GROUND_TRUTH_FOLDER> <OUTPUT_FOLDER> [DOCKER_IMAGE_TAG]
```

    - INPUT_FOLDER: Path to the folder containing predictions (should follow structure above).
    - GROUND_TRUTH_FOLDER: Folder containing mapping.csv and ground truth JSON files (should follow structure above).
    - OUTPUT_FOLDER: Where evaluation results are saved.
    - DOCKER_IMAGE_TAG: (optional) Defaults to unicorn_eval.
